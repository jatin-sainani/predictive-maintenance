# -*- coding: utf-8 -*-
"""predictive_maintenance_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VHIpoZ-36MdN7WgZk0LwmtO_QxT5EV4T
"""

#Description: This program classifies machines as requiring maintenance or not
#                       using Artificial Neural Networks (ANN)

#import libraries
import glob
from keras.models import Sequential, load_model
import numpy as np
import pandas as pd
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import matplotlib.pyplot as plt
import keras as k
import datetime

#Load the data
from google.colab import files
uploaded = files.upload()

df = pd.read_csv('predictive_machine.csv')

#Print the first 5 rows
df.head()

df.shape

#Creating a list of column names to keep
columns_to_retain = ['temp','pf','sd','curr','demo','vib','rc','htn','classification']

#Drop the columns that are not wanted in columns_to_retain
df = df.drop( [col for col in df.columns if not col in columns_to_retain] , axis=1 )

#Drop the rows with na or missing values
df = df.dropna(axis=0)

#Transform the non-numeric data in the columns
for column in df.columns:
  if df[column].dtype == np.number:
    continue
  df[column] = LabelEncoder().fit_transform( df[column] )

#print the first 5 rows of the new cleaned data set
df.head()

#Split the data into independent (X) data set -features  and (Y) data set -targets
X = df.drop(['classification'], axis=1)
Y = df['classification']

#Feature Scaling
#min-max scaler method scales the data set so that all the input features lie between 0 and 1
X_scaler = MinMaxScaler()
X_scaler.fit(X)
column_names = X.columns
X[column_names] = X_scaler.transform(X)

#Split the data into 80% training and 20% testing and Shuffle 
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle=True)

#Build the model
model = Sequential()
model.add( Dense(256, input_dim= len(X.columns) , kernel_initializer= k.initializers.random_normal(seed=13), activation='relu') )
model.add( Dense(1, activation='hard_sigmoid') )

#Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#Train the model
history = model.fit(X_train, Y_train, epochs = 2000, batch_size= X_train.shape[0])

#Saving the model
model.save('ckd.model')

#Visualize the models loss and accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['loss'])
plt.title('model accuracy and loss')
plt.ylabel('accuracy and loss')
plt.xlabel('epoch')

print('shape of training data:', X_train.shape)
print('shape of test data:', X_test.shape)

pred = model.predict(X_test)
pred

#defining threshold as 0.5
pred = model.predict(X_test)
pred = [1 if y>=0.5 else 0 for y in pred]
pred

#Show the actual values
Y_test

#Show the actual and predicted values
print('Original : {0}'.format(", ".join(str(x) for x in Y_test)))
print('Predicted : {0}'.format(", ".join(str(x) for x in pred)))

x = datetime.datetime.now()
print("Today's date: ")
print(x.strftime("%x"))

from datetime import date
from dateutil.relativedelta import relativedelta

six_months = date.today() + relativedelta(months=+6)
print("Scheduled Maintenance Date: ")
print(six_months.strftime("%x"))

from datetime import timedelta
n = (Y_test == 1).sum()
days_to_subtract = n/24 + 1
d = six_months - timedelta(days=days_to_subtract)
print("Predicted Maintenance Date: ")
print(d.strftime("%x"));

p = (24-n)/2400
print("Life of the machine is degraded by: ")
print(p)